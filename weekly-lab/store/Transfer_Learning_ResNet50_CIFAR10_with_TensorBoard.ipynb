{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e8e73ef1-8ad0-45a5-a7f5-b9f10d50fe09",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "# Transfer Learning with ResNet-50 on CIFAR-10 using Keras and TensorBoard\n",
        "\n",
        "Transfer learning is a machine learning technique where a model developed for a task is reused as the starting point for a model on a second task. In this notebook, we'll demonstrate transfer learning using the ResNet-50 architecture on the CIFAR-10 dataset.\n",
        "\n",
        "## Steps:\n",
        "1. **Data Loading and Preprocessing:** We'll start by loading the CIFAR-10 dataset and preprocess the images to make them suitable for the ResNet-50 model.\n",
        "\n",
        "2. **Model Building:** We'll use the ResNet-50 model pre-trained on the ImageNet dataset. We'll remove the top (classification) layer and add our custom layers suitable for CIFAR-10 classification.\n",
        "\n",
        "3. **Training:** Initially, we'll freeze the layers of the ResNet-50 model and train only our custom layers. This is to prevent large gradient updates from ruining the pre-trained weights of the ResNet-50 model.\n",
        "\n",
        "4. **Fine-tuning:** After initial training, we'll unfreeze some of the deeper layers of the ResNet-50 model and continue training. This allows our model to make minor adjustments to the deeper features to better suit our dataset.\n",
        "\n",
        "5. **Monitoring with TensorBoard:** We'll use TensorBoard to monitor the training process. TensorBoard provides visual insights into the events happening during training, which can be helpful for debugging and optimization purposes.\n",
        "\n",
        "Let's begin!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "103a2625-dbba-479f-8e03-c1a3bbfe8e55",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import TensorBoard"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afe6d6ce",
      "metadata": {},
      "source": [
        "#### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6291eab2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load CIFAR-10 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "train_images = preprocess_input(train_images)\n",
        "test_images = preprocess_input(test_images)\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abeaf4f7-7f45-4fde-b3bf-e169bccb169f",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Model Building\n",
        "\n",
        "We'll now build our model. The steps are as follows:\n",
        "\n",
        "1. **Load the ResNet-50 Model:** We'll use the ResNet-50 model pre-trained on the ImageNet dataset. This gives us a powerful feature extractor that we can leverage for our CIFAR-10 classification task.\n",
        "\n",
        "2. **Add Custom Layers:** We'll add a global average pooling layer followed by a dense layer with 1024 units and ReLU activation. The final layer will be a dense layer with 10 units (for the 10 CIFAR-10 classes) and a softmax activation to output class probabilities.\n",
        "\n",
        "3. **Compile the Model:** We'll compile the model using the Adam optimizer and the categorical crossentropy loss function. We'll also monitor accuracy during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "99d2af05-58fe-4e9f-ae5d-13e0dd1e342d",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "# Load the ResNet-50 model with imagenet weights\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add custom layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(10, activation='softmax')(x)\n",
        "\n",
        "# Create the full model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze all layers of the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c7b95bd-4241-493b-95c9-783223f01dfc",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "With our model built, we can now proceed to train it. We'll start by training only the custom layers we added, while keeping the ResNet-50 layers frozen. This is because the ResNet-50 layers already contain useful features (from being trained on ImageNet), and we don't want to modify them too much in the initial stages of training.\n",
        "\n",
        "We'll also use TensorBoard to monitor our training in real-time. TensorBoard provides a suite of visualization tools to help understand, debug, and optimize the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e379ae05-3436-4114-9565-de38544d75c7",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "# Resize images to 224x224 (required input size for ResNet-50)\n",
        "train_images_resized = np.array([keras.preprocessing.image.array_to_img(img, scale=False).resize((224, 224)) for img in train_images])\n",
        "test_images_resized = np.array([keras.preprocessing.image.array_to_img(img, scale=False).resize((224, 224)) for img in test_images])\n",
        "\n",
        "# TensorBoard callback\n",
        "tensorboard_callback = TensorBoard(log_dir='./logs/resnet', histogram_freq=1, write_graph=True, write_images=True)\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images_resized, train_labels, epochs=5, batch_size=32, validation_data=(test_images_resized, test_labels), callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9971a27d-313e-4409-be1e-efffb3d9a133",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Fine-tuning the Model\n",
        "\n",
        "After the initial training, we can proceed to fine-tune the model. Fine-tuning involves unfreezing some of the deeper layers of the ResNet-50 model and continuing the training. This allows the model to adjust the pre-trained features slightly to better fit our specific dataset.\n",
        "\n",
        "For this demonstration, we'll unfreeze the last 5 layers of the ResNet-50 model and continue training. Remember to always use a lower learning rate during fine-tuning to ensure that the model converges smoothly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4a2fb6d-f5e8-4720-b39f-e0e3580e502f",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "# Unfreeze the last 5 layers of the base model\n",
        "for layer in base_model.layers[-5:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Use a lower learning rate\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Continue training the model\n",
        "model.fit(train_images_resized, train_labels, epochs=5, batch_size=32, validation_data=(test_images_resized, test_labels), callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1850619-c729-4900-9ab8-7d596c4e5888",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Monitoring with TensorBoard\n",
        "\n",
        "TensorBoard is a powerful tool provided by TensorFlow to visualize the training process. It offers a suite of visualization tools to understand, debug, and optimize the training.\n",
        "\n",
        "To view the training progress and other insights, you can run TensorBoard by pointing it to the log directory we specified (`./logs`). This can be done using the command:\n",
        "\n",
        "```\n",
        "tensorboard --logdir=./logs\n",
        "```\n",
        "\n",
        "Once TensorBoard is running, you can navigate to the provided URL in your web browser to view the visualizations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbeec1a1-1461-4fdd-aa24-6dde8350d288",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Visual Comparison of Predictions\n",
        "\n",
        "To visually compare the model's predictions with the actual labels, we'll use `matplotlib`. We'll select a few random images from the test set, make predictions using our trained model, and then plot the images with the predicted and actual labels side by side for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eaca190-feba-43c7-941b-fee8f1038998",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define CIFAR-10 class names\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Select a few random images from the test set\n",
        "num_samples = 10\n",
        "indices = np.random.choice(len(test_images_resized), size=num_samples, replace=False)\n",
        "sample_images = test_images_resized[indices]\n",
        "sample_labels = test_labels[indices]\n",
        "\n",
        "# Make predictions using the model\n",
        "predictions = model.predict(sample_images)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "actual_labels = np.argmax(sample_labels, axis=1)\n",
        "\n",
        "# Plot the images with predicted and actual labels\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i, (img, pred_label, actual_label) in enumerate(zip(sample_images, predicted_labels, actual_labels)):\n",
        "    plt.subplot(2, num_samples // 2, i + 1)\n",
        "    plt.imshow(img.astype('int32'))\n",
        "    plt.title(f'Pred: {class_names[pred_label]}\\nActual: {class_names[actual_label]}')\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "noteable": {
      "last_delta_id": "895ac641-48e9-4305-a5f0-45626fd59265"
    },
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "ddb162f8-eec6-589c-8f80-6128a4e8df30",
        "openai_ephemeral_user_id": "37fccdf7-e356-596e-9174-5aeb4cca01a9",
        "openai_subdivision1_iso_code": "US-CA"
      }
    },
    "nteract": {
      "version": "noteable@2.9.0"
    },
    "selected_hardware_size": "small"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
